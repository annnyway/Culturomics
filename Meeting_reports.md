# 10.11.2018

### О чем будет наш проект

Google N-Gram Viewer – наборы н-грамм на 8-9 языках, есть русский
Там н-граммы только те, у которых частотность больше 40
Контексты н-грамм смотреть нельзя, смотреть можно общие цифры

Проблемы с русским языком: качество распознавания плохое
Ошибки после 1918 года – не будем исправлять

До 1918 года – будем исправлять. Эти символы не распознаются гуглом

### Задача 1. Скачать все русские н-граммы – сотни гигабайт (придумать, как)

Csv-файлы: 
- Они разбиты по первым двум буквам н-граммы
- Файлы с кириллическими словами начинаются с латиницы
- вид табличек: Н-грамма Tab год ее появления в книжке Tab количество за этот год. Н-граммы повторяются. 

Потом нужно извлечь из общего набора те н-граммы, что до 1918 года. 

Нам их надо исправить.

Проверка и коррекция орфографии – половина задачи. 

Изучить современные способы проверки орфографии! Скорее всего, машинное обучение – учиться на старинных текстах в Инете.  Есть распознанные сканы и тексты на старой орфографии. Аккаунты в ЖЖ: 
1. fhiltrius, Любжин, работает в библиотеке МГУ, доктор фил наук. 
2. platonicus 
Они пишут тексты и комментарии в старой орфографии. 

Результат проекта:
- выложить в исправленном виде открытый датасет

Как почистить от шума?
- можно лемматизировать и сравнить со словарем (предложение Марины)

### Задача 2. Применение статистических методов к n-грамм 20 века. 

Частотность слов связана с событиями культуры и истории

- изучить поведение отдельных н-грамм
- можем посчитать выборочные н-граммы между собой

Построить машиночитаемый датасет про отдельные н-граммы, выяснить, какие есть странности в поведении – выбросы, резкий рост и падение. Поиск выбросов! У нас только 1 фактор – частотность, и надо понять, какие пики и спады могут быть интересными с точки зрения культуромики. 

Изучить, как работают временные ряды в статистике! 

### Задача 3. Мы можем сопоставить что-то из н-грамм и линкед опэн дэта (онтологии, они делаются автоматически из Википедии, где информация разбита на категории статей для машин). 

На основе статей из вики созданы базы данных; есть понятные полей описания: если город, то число жителей и т.д. Это описание формализовано и кладется в базу. Мы можем задать вопрос базе: «выдай все координаты городов, где рождались композиторы». Нам надо извлекать поля описания с датами, чтобы сопоставлять их с датами н-грамм. 

Язык работы с такими базами данных - Sparql – расширение SQL.

### Результат проекта – список курьёзов, которые мы ожидали и не ожидали. 

Только 20-й век? 

Ход работы: нам нужно разделиться и каждой выбрать сферу ответственности
Один человек – старая орография (спелчекер под эти данные), другой - статистика, третий – линкед опэн дэйта. 


--- 

Темы возможных статей: 

1. Исправление орфографии – техническая работа, не очень научная, но полезная. Какие методы из существующих можно использовать, с какими проблемами сталкивались, и что получилось. 

2. «Культуромика ковровой бомбардировкой»: о результатах статистики и связывания с linked open data. О том, что мы взяли материал для культуромики и зашли не со стороны частных сюжетов, а со стороны того, что там ведет себя странно. 
Введение про то, что такое культуромика, какие есть исследования. Потом – данные, как мы с ними работали. Третье – методы: поиск выбросов и работа с временными рядами, взаимодействие с линк опэн дэйта. Примеры выбросов и интерпретация результатов, площадка для дискуссии.
 
Что почитать: 
> А. Бонч-Осмоловская, публикация по культуромике в НКРЯ.
> История и теория проверки и исправления орфографии.  
> Google scholar 
> Sparkle – библиотека внутри питона


Можем встречаться с куратором по субботам до 13:30 или скайп по вечерам субботы


# 18.11.2018  - cозвон по скайпу 

Мы договорились, что Марина М. берет на себя спеллчекер для старой орфографии, Марина П. занимается статистикой (поиск выбросов, анализ временный рядов), а Аня будет работать над связыванием n-грамм с Wikidata (здесь тоже может понадобиться статистика, но работать нужно будет не столько с выбросами, сколько с незначительными временными скачками вверх; возможно, тут тоже будет нужен анализ временных рядов).

По поводу статистики и связи с Wikidata - пока рассматриваем русские n-граммы только после 1918 года. Когда будет готов спеллчекер и если позволит время, можно будет применить статистику и к словам до 1918 года. 

Уточнение по результатам проекта. У нас будет 2 результата и, соответственно, 2 основные задачи: датасет с исправленными n-граммами и исследование того, что происходит в n-граммах с точки зрения статистики. Вторая задача пока не предполагает создание программы или ресурса, она больше исследовательская, но мы можем придумать в этом году какой-нибудь интерфейс, который, например, в реальном времени будет соединять n-граммы с Wikidata. Или мы создадим свой поиск по русским n-граммам, аналогичный гугловскому. Об этом нужно подумать.

Как связывать с Wikidata: надо попробовать поработать с разными видами n-грамм: пентограммы или униграммы могут не подойти. Нужно взять н-граммы, лемматизировать их и смотреть, представлены ли они как объекты в Wikidata. Если представлены, то смотрим, какие есть даты у этого объекта в Wikidata и пробуем связать с датами n-грамм. Что почитать про Wikidata: нужно отталкиваться от понятия онтологий, но здесь больше практической части (это как курсы стрижки :). 

То же самое для спеллчекера: с униграммами модель будет работать хуже, чем с тетраграммами. Также мы подумали о том, где еще взять тексты на старом русском для машинного обучения: можно использовать программы по переводу текстов в современной орфографии в старую и таким образом собрать большую коллекцию.  

Статистика. Вопрос: как членить данные во времени. Тут понадобятся кривые сглаживания/скользящие значения. 

Еще про проблемы проекта:
- нужно как-то отсеивать из н-грамм издательскую информацию, ненарративные книги (телефонные справочники, словари, энциклопедии). Вроде бы на эту тему что-то писал В.И.Беликов
- старая орфография не едина: есть гротовская орфография и разные диалектные традиции. Нам надо ориентироваться на гротовскую; возможно, перевести диалектные особенности в гротовскую.  
- мы не знаем, как Google Ngrams рассчитывали относительные значения частотности. Нам надо понять, как они считали, где брали абсолютные значения. 

Время встреч с куратором перенесено на понедельник. 

# 14.01.2019

Разговаривали в основном про связь n-грамм с Викиданными. На текущий момент были построены запросы на SPARQL к Викиданным для извлечения дат для отдельных сущностей. Теперь задача в том, как привязать к сущностям в n-граммах Q-идентификаторы из Викиданных. 

Договорились, что не совсем оптимально использовать Solr Text Tagger для разметки n-грамм этими идентификаторами - это будет медленно. Лучше использовать систему словарей: ключами будут сущности из Викиданных, а значениями - их Q-идентификаторы. Мы будем сопоставлять n-граммы с этими словарями, присваивать им номера Q, затем прогонять все полученные Q через SPARQL-запросы, таким образом вытаскивая даты для всех n-грамм. Затем мы будем сравнивать эти даты с датами n-грамм и смотреть, какие события отражаются в изменении частотности n-грамм. Эти словари будут составлены из дампа Викиданных; нужно продумать их структуру: что именно будет храниться в разных словарях: однословные, многословные сущности, леммы и т.п.  

Договорились, что Аня получит доступ к серверу для работы с дампом. 

В целом по проекту: мы решили, что лемматизировать все n-граммы для статистического анализа и для связывания с Викиданными не нужно. Нужно сначала собрать все словоформы в n-граммах, их лемматизировать и положить в словарь "словоформа-лемма", и уже потом еще раз прогнать n-граммы через этот словарь - так получится быстро лемматизировать весь датасет. 

