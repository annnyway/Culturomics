{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssivmpxL_NMQ"
   },
   "outputs": [],
   "source": [
    "# Подключаем библиотеки\n",
    "import mysql.connector\n",
    "import re\n",
    "from google_ngram_downloader import readline_google_store # библиотека для выгрузки нграм\n",
    "from pymystem3 import Mystem\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем соединение с сервером\n",
    "password = input(\"Enter your password: \")\n",
    "con=mysql.connector.connect(host=\"localhost\", database=\"google_ngrams\", user=\"root\", password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h57Ba-wjlfaF"
   },
   "outputs": [],
   "source": [
    "# создаем курсор для отправки запросов и получения результатов\n",
    "cur=con.cursor(dictionary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19l9qllmh-f3"
   },
   "source": [
    "#### Как выглядят данные, которые мы будем класть в базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "B0nxEUOJh8qh",
    "outputId": "0a4a0c6c-35fe-4a8c-f6a6-58eeee367f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Allocation, 1960, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "fname, url, records = next(readline_google_store(ngram_len=1, lang='rus', indices='a'))\n",
    "record = next(records)\n",
    "request = \"({}, {}, {}, {})\".format(record.ngram,\n",
    "                                    record.year,\n",
    "                                    record.match_count,\n",
    "                                    record.volume_count)\n",
    "print(request)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RHyvPESkOcA"
   },
   "source": [
    "#### И таких все слова на заданную букву"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7oTcXpgRZiL"
   },
   "outputs": [],
   "source": [
    "# индексы униграм (символы, с которых они начинаются)\n",
    "unigram_indices = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "                   'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
    "                   'other', 'pos', 'punctuation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juq0vMzemoLe"
   },
   "source": [
    "#### создадим необходимые таблицы:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hz4PFT9HmtPt"
   },
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE TABLE `uni_grams_after_1918` (`idx` text NOT NULL, `raw_n_gram` text NOT NULL, `n_gram` text NOT NULL, `year` int NOT NULL, `match_count` int NOT NULL, `volume_count` int NOT NULL)\")\n",
    "cur.execute(\"CREATE TABLE `uni_grams_before_1918` (`idx` text NOT NULL, `raw_n_gram` text NOT NULL, `n_gram` text NOT NULL, `year` int NOT NULL, `match_count` int NOT NULL, `volume_count` int NOT NULL, `new_idx` text DEFAULT NULL, `is_bastard` boolean DEFAULT false, `new_ngram` text DEFAULT NULL)\")\n",
    "cur.execute(\"CREATE TABLE `vocabulary_before_1918` (`uni_gram` text NOT NULL, `match_count` int NOT NULL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N974h1todlU"
   },
   "source": [
    "#### Первая нужна для статистического анализа, вторая -  для исправления орфографии, третья - для хранения словаря дореволюционной орфографии "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N3R7q5z_mzK"
   },
   "outputs": [],
   "source": [
    "# функция,выгружающая нграмы до и после 1918 года в разные таблицы\n",
    "# на вход подаются параметры, определяющие длину нграм, язык и индекс\n",
    "\n",
    "def load_ngrams(table_name, my_len, my_lang, my_indices, before_1918=False):\n",
    "    fname, url, records = next(readline_google_store(ngram_len=my_len, lang=my_lang, indices=my_indices))\n",
    "    record = next(records)\n",
    "    e = 0\n",
    "    while True:\n",
    "        try:\n",
    "            if before_1918:\n",
    "                if record.year >= 1918:\n",
    "                    record = next(records)\n",
    "                elif record.year < 1918:\n",
    "                    ngram = record.ngram\n",
    "                    request = \"INSERT INTO \" + table_name + \"(idx, raw_n_gram, n_gram, year, match_count, volume_count)\"\n",
    "                    request += \" VALUES ('{}', '{}', '{}', {}, {}, {});\".format(my_indices,\n",
    "                                                             ngram.replace(\"'\",\"\\\\\\'\"),\n",
    "                                                             normalize(ngram).replace(\"'\",\"\\\\\\'\"),\n",
    "                                                             record.year,\n",
    "                                                             record.match_count,\n",
    "                                                             record.volume_count)\n",
    "                    cur.execute(request)\n",
    "                    con.commit()\n",
    "                    if e%1000==0:\n",
    "                        print('loaded: ' + str(e))  # чтобы видеть количество загрузок\n",
    "                    e += 1\n",
    "                    record = next(records)\n",
    "            else:\n",
    "                if record.year < 1918:\n",
    "                    record = next(records)\n",
    "                elif record.year >= 1918:\n",
    "                    ngram = record.ngram\n",
    "                    request = \"INSERT INTO \" + table_name + \"(idx, raw_n_gram, n_gram, year, match_count, volume_count)\"\n",
    "                    request += \" VALUES ('{}', '{}', '{}', {}, {}, {});\".format(my_indices,\n",
    "                                                                 ngram.replace(\"'\",\"\\\\\\'\"),\n",
    "                                                                 normalize(ngram).replace(\"'\", \"\\\\\\'\"),\n",
    "                                                                 record.year,\n",
    "                                                                 record.match_count,\n",
    "                                                                 record.volume_count)\n",
    "                    cur.execute(request)\n",
    "                    con.commit()\n",
    "                    if e%1000==0:\n",
    "                        print('loaded: ' + str(e)) # чтобы видеть количество загрузок\n",
    "                    e += 1\n",
    "                    record = next(records)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            print(\"StopIteration\")\n",
    "    \n",
    "# функция, нормализующая нграмы:\n",
    "# избавление от тэгов\n",
    "# замена нулей в словах на букву \"о\"\n",
    "# замена единиц в словах на букву \"i\"\n",
    "\n",
    "def normalize(ngram):\n",
    "    if '_' in ngram:\n",
    "        index = ngram.find('_')\n",
    "        ngram = ngram[: index]\n",
    "#   if not re.sub(\"[.,:\\'-]\", '', ngram).isdigit():\n",
    "#     ngram = re.sub(\"0\", 'О', ngram)\n",
    "#     ngram = re.sub(\"1\", 'i', ngram)\n",
    "    return(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Загрузка данных в первую таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "load_ngrams(\"uni_grams_before_1918\",1, 'rus', 'a', before_1918=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Загрузка данных во вторую таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "load_ngrams('uni_grams_after_1918', 1, 'rus', 'a', before_1918=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Создадим словарь дорефолюционного русского языка. Для этого возьмем имеющийся корпус текстов с нужной орфографией, полученный с данных викитеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import punkt # для сегментации предложений\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re #для замены ненужных символов\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabMaker(object):\n",
    "    def __init__(self, corpus, min_freq=5):\n",
    "        self.corpus = self.read_corpus(corpus)\n",
    "        self.corpus_words, self.num_corpus_words, self.counts = self.distinct_words(min_freq)\n",
    "        self.index2word, self.word2index = self.build_index()\n",
    "               \n",
    "    def read_corpus(self, filename, min_freq=20):\n",
    "        print('Reading a corpus')\n",
    "        if str(filename).endswith('.txt'):\n",
    "            file = open(filename, 'r', encoding = 'utf-8')\n",
    "            return([[ re.sub('{[^}]}', '', w.lower()) for w in nltk.tokenize.WordPunctTokenizer().tokenize(sentence)] \n",
    "                    for sentence in sent_tokenize(file.read(), language='slovene')])\n",
    "            file.close()\n",
    "            print('Done!')\n",
    "        else:\n",
    "            return(filename)\n",
    "    \n",
    "    def distinct_words(self, min_freq):\n",
    "        print('Estimating cospus size')\n",
    "        flat_corpus = [word for sentence in self.corpus for word in sentence]\n",
    "        counts = Counter(flat_corpus)\n",
    "        corpus_words = sorted(list(set([word for word in flat_corpus])))\n",
    "        num_corpus_words = len(corpus_words)\n",
    "        return(corpus_words, num_corpus_words, counts)\n",
    "        print('Done!')\n",
    "        print('You can checkout the number of corpus words and the words themselves with commands .num_corpus_words, .corpus_words')\n",
    "    \n",
    "    def build_index(self):\n",
    "        word2index = dict()\n",
    "        for word in self.corpus_words:\n",
    "            word2index[word] = len(word2index)\n",
    "            word2index['UNK'] = 0\n",
    "        return ({index:word for word, index in word2index.items()}, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading a corpus\n",
      "Estimating cospus size\n"
     ]
    }
   ],
   "source": [
    "VM = VocabMaker(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['спасъ',\n",
       " 'на',\n",
       " 'бору',\n",
       " ',',\n",
       " 'въ',\n",
       " 'московскомъ',\n",
       " 'кремлѣ',\n",
       " '/',\n",
       " 'до',\n",
       " '358011',\n",
       " 'о',\n",
       " 'красотѣ',\n",
       " 'бога',\n",
       " '(',\n",
       " 'вивекананда',\n",
       " '/',\n",
       " 'рудникова',\n",
       " ')/',\n",
       " 'до',\n",
       " '434173',\n",
       " 'о',\n",
       " 'верхне',\n",
       " '-',\n",
       " 'иргизском',\n",
       " 'монастырѣ',\n",
       " ',',\n",
       " 'о',\n",
       " 'пріѣздѣ',\n",
       " 'въ',\n",
       " 'него',\n",
       " 'генерала',\n",
       " 'рунича',\n",
       " ',',\n",
       " 'пожалованіи',\n",
       " 'императора',\n",
       " 'павла',\n",
       " '1',\n",
       " '-',\n",
       " 'го',\n",
       " 'на',\n",
       " 'возобновленіе',\n",
       " 'сгорѣвшей',\n",
       " 'в',\n",
       " 'немъ',\n",
       " 'церкви',\n",
       " 'къ',\n",
       " 'числу',\n",
       " 'любопытныхъ',\n",
       " 'историческихъ',\n",
       " 'воспоминаній',\n",
       " 'о',\n",
       " 'томъ',\n",
       " 'времени',\n",
       " 'должно',\n",
       " 'отнести',\n",
       " 'пребываніе',\n",
       " 'въ',\n",
       " 'петергофѣ',\n",
       " 'лѣтомъ',\n",
       " '1797',\n",
       " 'года',\n",
       " 'бывшаго',\n",
       " 'польскаго',\n",
       " 'короля',\n",
       " 'станислава',\n",
       " '-',\n",
       " 'августа',\n",
       " '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VM.corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VM.num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1116367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VM.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punct = punctuation+'«»—…“”*№–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_corpus = [word.strip(punct) for sentence in VM.corpus for word in sentence if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(corpus, n):\n",
    "    # Склеивание токенов в нграмы\n",
    "    ngrams = zip(*[corpus[i:] for i in range(n)])\n",
    "    joined = [\" \".join(ngram) for ngram in ngrams]\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['41382 письмо ѳёдора михайловича достоевскаго',\n",
       " 'письмо ѳёдора михайловича достоевскаго любови',\n",
       " 'ѳёдора михайловича достоевскаго любови ѳедоровнѣ',\n",
       " 'михайловича достоевскаго любови ѳедоровнѣ достоевской',\n",
       " 'достоевскаго любови ѳедоровнѣ достоевской 25',\n",
       " 'любови ѳедоровнѣ достоевской 25 ']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(flat_corpus[:10], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Воспользуемся сначала только униграмами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_grams = generate_ngrams(flat_corpus, 1)\n",
    "counts = Counter(uni_grams)\n",
    "unique_uni_grams = sorted(list(set([word for word in uni_grams])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474279"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_uni_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим данные в базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "e=0\n",
    "for word in uni_grams:\n",
    "    word.replace('\"', '\\\\\"')\n",
    "    request = \"INSERT INTO vocabulary_before_1918(uni_gram, match_count)\"\n",
    "    request += \" VALUES ('{}', {});\".format(word.replace(\"'\",\"\\\\\\'\"),\n",
    "                                            counts[word])\n",
    "    cur.execute(request)\n",
    "    con.commit()\n",
    "    if e%1000==0:\n",
    "        print('loaded: ' + str(e)) # чтобы видеть количество загрузок\n",
    "    e += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Отметим нграмы из \"uni_grams_before_1918\", не соответсвующие значениям словаря\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Меняем дефолтное значение в колонке \"is_bastard\" если слова нет в словаре\n",
    "\n",
    "req=\"\"\"UPDATE uni_grams_before_1918\n",
    "SET is_bastard=true\n",
    "WHERE n_gram NOT IN \n",
    "(SELECT uni_gram FROM vocabulary_before_1918)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(req)\n",
    "present=cur.fetchall()\n",
    "con.commit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of RusNgramsTill1918.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
