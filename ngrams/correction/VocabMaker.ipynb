{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import punkt # для сегментации предложений\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re #для замены ненужных символов\n",
    "from collections import Counter\n",
    "import re\n",
    "from string import punctuation\n",
    "punct = punctuation+'«»—…“”*№–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabMaker(object):\n",
    "    def __init__(self, corpus, min_freq=5):\n",
    "        self.corpus = self.read_corpus(corpus)\n",
    "        self.corpus_words, self.num_corpus_words, self.counts = self.distinct_words(min_freq)\n",
    "        self.index2word, self.word2index = self.build_index()\n",
    "               \n",
    "    def read_corpus(self, filename, min_freq=20):\n",
    "        print('Reading a corpus')\n",
    "        if str(filename).endswith('.txt'):\n",
    "            file = open(filename, 'r', encoding = 'utf-8')\n",
    "            return([[ re.sub('{[^}]}', '', w.lower()) for w in nltk.tokenize.WordPunctTokenizer().tokenize(sentence)] \n",
    "                    for sentence in sent_tokenize(file.read(), language='slovene')])\n",
    "            file.close()\n",
    "            print('Done!')\n",
    "        else:\n",
    "            return(filename)\n",
    "        \n",
    "    def not_russian(self, string):\n",
    "        s = re.sub(\"[.,:\\'-]\", '', string)\n",
    "        charRe = re.compile(r'[a-zA-Z0-9.]')\n",
    "        st = charRe.search(s)\n",
    "        return bool(st) or bool(re.search(r'\\d', s) or bool(re.search(r'[^a-zа-яё ]+', string)))\n",
    "    \n",
    "    def distinct_words(self, min_freq):\n",
    "        print('Estimating cospus size')\n",
    "        flat_corpus = [word for sentence in self.corpus for word in sentence]\n",
    "        counts = Counter(flat_corpus)\n",
    "        corpus_words = sorted(list(set([word for word in flat_corpus if not self.not_russian(word)])))\n",
    "        num_corpus_words = len(corpus_words)\n",
    "        \n",
    "        return(corpus_words, num_corpus_words, counts)\n",
    "        print('Done!')\n",
    "        print('You can checkout the number of corpus words and the words themselves with commands .num_corpus_words, .corpus_words')\n",
    "    \n",
    "    def build_index(self):\n",
    "        word2index = dict()\n",
    "        for word in self.corpus_words:\n",
    "            word2index[word] = len(word2index)\n",
    "            word2index['UNK'] = 0\n",
    "        return ({index:word for word, index in word2index.items()}, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VM = VocabMaker(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocabulary.txt', 'a') as f:\n",
    "#     for word in VM.corpus_words:\n",
    "#         f.write(\"{}\\n\".format(word))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = open('vocabulary.txt', 'r').read()\n",
    "# vocab = []\n",
    "# for line in vocabulary.split(\"\\n\"):\n",
    "#     vocab.append(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
